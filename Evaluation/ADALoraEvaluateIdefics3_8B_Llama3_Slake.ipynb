{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11df418f147e4e72801052240fedce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bfbe4bc5f3e49a8a477cbeb755ce9be",
              "IPY_MODEL_046dfa6258754a2babd127033863d6a5",
              "IPY_MODEL_a38e2c478236470b8d1045d37ac88e5f"
            ],
            "layout": "IPY_MODEL_2666703e5b464f9a9837c81fd6b18539"
          }
        },
        "2bfbe4bc5f3e49a8a477cbeb755ce9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fed5dda334c442548722702c718bbf7d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ab9d4fb6d4b54be7adb1c1cff980678e",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "046dfa6258754a2babd127033863d6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3506eb4c55ac431ab9a7381511283568",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66f90b6da3c44f5798c377c55f2228ce",
            "value": 4
          }
        },
        "a38e2c478236470b8d1045d37ac88e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f46fbcf7ff41e584e566c784945349",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cced13b9e87a4bf4a8881b31a2c52ca6",
            "value": "â€‡4/4â€‡[01:26&lt;00:00,â€‡20.00s/it]"
          }
        },
        "2666703e5b464f9a9837c81fd6b18539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed5dda334c442548722702c718bbf7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9d4fb6d4b54be7adb1c1cff980678e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3506eb4c55ac431ab9a7381511283568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f90b6da3c44f5798c377c55f2228ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35f46fbcf7ff41e584e566c784945349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cced13b9e87a4bf4a8881b31a2c52ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GAkpUr1n9JkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpVsQ_DwDQ7Q",
        "outputId": "16438c78-35fa-4bc0-995a-4b8bbd737f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Øº\n",
            "Invalid input. Must be one of ('y', 'yes', '1', 'n', 'no', '0', '')\n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `1`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForVision2Seq\n",
        "\n",
        "import torch\n",
        "\n",
        "# Model and adapter names\n",
        "base_model_name = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
        "adapters_name = \"Dtarget/adallaslake\"\n",
        "\n",
        "# QLoRA configuration\n",
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"bnb_4bit_compute_dtype\": torch.float16\n",
        "}\n",
        "\n",
        "# Load the base model with quantization\n",
        "modelF = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# Load AdaLoRA adapters\n",
        "modelF = PeftModel.from_pretrained(modelF, adapters_name)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "11df418f147e4e72801052240fedce2e",
            "2bfbe4bc5f3e49a8a477cbeb755ce9be",
            "046dfa6258754a2babd127033863d6a5",
            "a38e2c478236470b8d1045d37ac88e5f",
            "2666703e5b464f9a9837c81fd6b18539",
            "fed5dda334c442548722702c718bbf7d",
            "ab9d4fb6d4b54be7adb1c1cff980678e",
            "3506eb4c55ac431ab9a7381511283568",
            "66f90b6da3c44f5798c377c55f2228ce",
            "35f46fbcf7ff41e584e566c784945349",
            "cced13b9e87a4bf4a8881b31a2c52ca6"
          ]
        },
        "id": "EQLmJiE1Q31u",
        "outputId": "3fe8015e-d8dd-44cd-9028-44d71cc21439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11df418f147e4e72801052240fedce2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForImageTextToText,AutoModelForVision2Seq\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"HuggingFaceM4/Idefics3-8B-Llama3\",\n",
        "    do_image_splitting=False\n",
        ")"
      ],
      "metadata": {
        "id": "NicVg4rfl-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "yDzr9xkMlgmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"mdwiratathya/SLAKE-vqa-english\")\n"
      ],
      "metadata": {
        "id": "_yOymNHThb0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, hf_dataset, processor):\n",
        "        self.dataset = hf_dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.dataset[idx]['question']\n",
        "        answer = self.dataset[idx]['answer']\n",
        "        image = self.dataset[idx]['image']\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"query\": {\"en\": question},\n",
        "            \"answers\": [answer]\n",
        "        }\n",
        "\n",
        "class MyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        images = []\n",
        "        for example in examples:\n",
        "            image = example[\"image\"]  # Directly use the PIL image\n",
        "            question = example[\"query\"]['en']\n",
        "            answer = random.choice(example[\"answers\"])\n",
        "\n",
        "            # Create messages as before\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": answer}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "            texts.append(text.strip())\n",
        "            images.append(image)\n",
        "\n",
        "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100  # Ignore padding in loss calculation\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = VQADataset(hf_dataset=dataset['test'], processor=processor)\n",
        "\n",
        "data_collator = MyDataCollator(processor=processor)"
      ],
      "metadata": {
        "id": "56l6H0_z6q_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from accelerate.utils import release_memory\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelF.to(device)\n",
        "\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "queries = []\n",
        "sampled_dataset = random.sample(list(test_dataset), 100)\n",
        "\n",
        "for example in tqdm(sampled_dataset, desc=\"Generating answers\"):\n",
        "    image = example[\"image\"]\n",
        "    query = example[\"query\"]\n",
        "    ground_truth = example[\"answers\"]\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": query}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text.strip()], images=[image], return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = modelF.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=32,  # Very short response\n",
        "                do_sample=False,   # Deterministic outputs\n",
        "                temperature=0.1,   # No randomness\n",
        "                top_p=0.5,         # Focus on top token\n",
        "                repetition_penalty=1.2  # Avoid repetitive outputs\n",
        "            )\n",
        "        release_memory(modelF)\n",
        "        generated_text = processor.batch_decode(\n",
        "            generated_ids[:, inputs[\"input_ids\"].size(1):], skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "    predictions.append(generated_text)\n",
        "    ground_truths.append(ground_truth)\n",
        "    queries.append(query)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Query\": queries,\n",
        "    \"Ground Truth\": ground_truths,\n",
        "    \"Prediction\": predictions\n",
        "})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ldLtjHL08_D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8c8e73-cbd8-472e-9ce3-68e254766f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating answers:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Generating answers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:07<00:00,  2.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "Qi4F307tWrnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from fuzzywuzzy import fuzz\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional, but sometimes needed\n",
        "\n",
        "\n",
        "# ---- Binary Accuracy Calculation ----\n",
        "binary_labels = [1 if \"yes\" in gt.lower() else 0 for gts in ground_truths for gt in gts]  # Convert to binary labels\n",
        "binary_preds = [1 if \"yes\" in pred.lower() else 0 for pred in predictions]  # Convert predictions to binary\n",
        "\n",
        "accuracy_binary = accuracy_score(binary_labels[:len(binary_preds)], binary_preds)\n",
        "\n",
        "print(f\"ðŸŒŸ Binary Accuracy: {accuracy_binary:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siqVfmW9NaBq",
        "outputId": "3eea34b8-8852-490f-da79-12d6d88106eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒŸ Binary Accuracy: 0.8300\n"
          ]
        }
      ]
    }
  ]
}