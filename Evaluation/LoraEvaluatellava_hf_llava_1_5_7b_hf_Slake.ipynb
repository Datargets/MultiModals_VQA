{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1831ff44b4b4459983ef25ab187df7bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fac72f1005445ac805715e0d80241db",
              "IPY_MODEL_a1f7b3c2791c4d25a37d917e4df4d308",
              "IPY_MODEL_3c11039498f643b8b02b8ec7061a4601"
            ],
            "layout": "IPY_MODEL_a1ba86733be54b58a1b3646ee5b9b44b"
          }
        },
        "2fac72f1005445ac805715e0d80241db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19156ada10104c94ad413901b565a816",
            "placeholder": "​",
            "style": "IPY_MODEL_a6dcbdcfb4e84ca8bdb085431486219c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a1f7b3c2791c4d25a37d917e4df4d308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3732e7ed4ca044d1bdbfdd035a16c61a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d0c8213758f48d68347377db46ae9fd",
            "value": 3
          }
        },
        "3c11039498f643b8b02b8ec7061a4601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e3c0382b3045718bf5a2cf30b5473d",
            "placeholder": "​",
            "style": "IPY_MODEL_4ad53a7d3e8a454f8d5cb6c4d4581d81",
            "value": " 3/3 [00:57&lt;00:00, 18.91s/it]"
          }
        },
        "a1ba86733be54b58a1b3646ee5b9b44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19156ada10104c94ad413901b565a816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6dcbdcfb4e84ca8bdb085431486219c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3732e7ed4ca044d1bdbfdd035a16c61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0c8213758f48d68347377db46ae9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3e3c0382b3045718bf5a2cf30b5473d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad53a7d3e8a454f8d5cb6c4d4581d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GAkpUr1n9JkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpVsQ_DwDQ7Q",
        "outputId": "8a232c51-04c0-487a-dc8b-3e6ef7364be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `1`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForImageTextToText,AutoModelForVision2Seq\n",
        "from peft import LoraConfig\n",
        "base_model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "adapters_name = \"Dtarget/llava20Fe\"\n",
        "\n",
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"bnb_4bit_compute_dtype\": torch.float16\n",
        "}\n",
        "\n",
        "# Load the base model with QLoRA for memory efficiency\n",
        "modelF = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config  # QLoRA for memory efficiency\n",
        ")\n",
        "\n",
        "modelF = PeftModel.from_pretrained(modelF, adapters_name)\n",
        "modelF = modelF.merge_and_unload()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "1831ff44b4b4459983ef25ab187df7bf",
            "2fac72f1005445ac805715e0d80241db",
            "a1f7b3c2791c4d25a37d917e4df4d308",
            "3c11039498f643b8b02b8ec7061a4601",
            "a1ba86733be54b58a1b3646ee5b9b44b",
            "19156ada10104c94ad413901b565a816",
            "a6dcbdcfb4e84ca8bdb085431486219c",
            "3732e7ed4ca044d1bdbfdd035a16c61a",
            "0d0c8213758f48d68347377db46ae9fd",
            "c3e3c0382b3045718bf5a2cf30b5473d",
            "4ad53a7d3e8a454f8d5cb6c4d4581d81"
          ]
        },
        "id": "EQLmJiE1Q31u",
        "outputId": "136f832b-e20d-456c-8f6f-78e61d4184ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1831ff44b4b4459983ef25ab187df7bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(modelF.peft_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgaiTPDH4FVU",
        "outputId": "fad23fb7-7813-4938-c4d0-010c101e8441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='llava-hf/llava-1.5-7b-hf', revision=None, inference_mode=True, r=8, target_modules={'k_proj', 'o_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights='gaussian', layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    do_image_splitting=False\n",
        ")"
      ],
      "metadata": {
        "id": "NicVg4rfl-jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d993686d-d436-42f2-88cf-d225c114930e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "yDzr9xkMlgmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"mdwiratathya/SLAKE-vqa-english\")\n"
      ],
      "metadata": {
        "id": "_yOymNHThb0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.dataset[idx]['question']\n",
        "        answer = self.dataset[idx]['answer']\n",
        "        image = self.dataset[idx]['image']  # Assuming it's a PIL image\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"query\": {\"en\": question},\n",
        "            \"answers\": [answer]\n",
        "        }\n",
        "\n",
        "\n",
        "# Define the MyDataCollator class\n",
        "class MyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "        self.image_token = '<image>'\n",
        "        self.end_of_utterance_token = '<end_of_utterance>'\n",
        "        self.pad_token_id = getattr(processor, 'pad_token_id', 0)  # Default to 0 if not found\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        for example in examples:\n",
        "            question = example[\"query\"]['en']\n",
        "            answer = random.choice(example[\"answers\"])\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "                        {\"type\": \"text\", \"text\": self.image_token},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": answer},\n",
        "                        {\"type\": \"text\", \"text\": self.end_of_utterance_token}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(str(item) for item in text)\n",
        "            else:\n",
        "                text = str(text)\n",
        "\n",
        "            texts.append(text.strip())\n",
        "\n",
        "        text_batch = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Prepare the batch dictionary\n",
        "        batch = {\n",
        "            \"input_ids\": text_batch[\"input_ids\"],\n",
        "            \"attention_mask\": text_batch[\"attention_mask\"]\n",
        "        }\n",
        "\n",
        "        # Create labels\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == self.pad_token_id] = -100  # Ignore padding in loss calculation\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = VQADataset(dataset=dataset['test'], processor=processor)\n",
        "\n",
        "data_collator = MyDataCollator(processor=processor)"
      ],
      "metadata": {
        "id": "56l6H0_z6q_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from accelerate.utils import release_memory\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelF.to(device)\n",
        "\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "queries = []\n",
        "sampled_dataset = random.sample(list(test_dataset), 100)\n",
        "# Generate text for each example\n",
        "for example in tqdm(sampled_dataset, desc=\"Generating answers\"):\n",
        "    image = example[\"image\"]\n",
        "    query = example[\"query\"]\n",
        "    ground_truth = example[\"answers\"]\n",
        "\n",
        "    messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "            {\"type\": \"text\", \"text\": processor.image_token},\n",
        "            {\"type\": \"text\", \"text\": query.get('en', '')}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text.strip()],\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        generated_ids = modelF.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=32,  # Very short response\n",
        "                do_sample=False,   # Deterministic outputs\n",
        "                temperature=0.1,   # No randomness\n",
        "                top_p=0.0,         # Focus on top token\n",
        "                repetition_penalty=1.2  # Avoid repetitive outputs\n",
        "            )\n",
        "\n",
        "        release_memory(modelF)\n",
        "        generated_text = processor.batch_decode(\n",
        "            generated_ids[:, inputs[\"input_ids\"].size(1):], skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "    predictions.append(generated_text)\n",
        "    ground_truths.append(ground_truth)\n",
        "    queries.append(query)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Query\": queries,\n",
        "    \"Ground Truth\": ground_truths,\n",
        "    \"Prediction\": predictions\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ldLtjHL08_D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22936294-70d9-4c7c-d8d5-f132bd0d4125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating answers:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Generating answers: 100%|██████████| 100/100 [03:39<00:00,  2.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyfjlvRyxFV_",
        "outputId": "892f9c94-33a2-4d8e-d5d1-5cd483e7fd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from fuzzywuzzy import fuzz\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional, but sometimes needed\n",
        "\n",
        "\n",
        "# ---- Binary Accuracy Calculation ----\n",
        "binary_labels = [1 if \"yes\" in gt.lower() else 0 for gts in ground_truths for gt in gts]  # Convert to binary labels\n",
        "binary_preds = [1 if \"yes\" in pred.lower() else 0 for pred in predictions]  # Convert predictions to binary\n",
        "\n",
        "accuracy_binary = accuracy_score(binary_labels[:len(binary_preds)], binary_preds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Binary Accuracy: {accuracy_binary:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQGmHuAzxIjC",
        "outputId": "e9e5e633-9990-444f-8d18-0b7db4a9b28e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Accuracy: 0.8800\n"
          ]
        }
      ]
    }
  ]
}