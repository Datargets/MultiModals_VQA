{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "98aa254ac2f4444486b76c305d430bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cae6f95410244a286c51090c9203f7f",
              "IPY_MODEL_c798fd277e7a428ba39332a5e38a1371",
              "IPY_MODEL_a41ad3888d7f4569aca9218c1885a791"
            ],
            "layout": "IPY_MODEL_df79a5d0669d4d7a86bd4ef1b1b30cc3"
          }
        },
        "5cae6f95410244a286c51090c9203f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e797ec7ef4041eabf20359bb85efb17",
            "placeholder": "​",
            "style": "IPY_MODEL_dd9acf6193474c0c8a37e531c35208b9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c798fd277e7a428ba39332a5e38a1371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14d8046ae2b46868c687d6743839aa1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_047ec5980d7542f0a0a6c47c6672859a",
            "value": 3
          }
        },
        "a41ad3888d7f4569aca9218c1885a791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b75b74a29ba4235a7959080b8908558",
            "placeholder": "​",
            "style": "IPY_MODEL_93093a306b534e5da605cdbe70c3116a",
            "value": " 3/3 [01:09&lt;00:00, 22.53s/it]"
          }
        },
        "df79a5d0669d4d7a86bd4ef1b1b30cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e797ec7ef4041eabf20359bb85efb17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9acf6193474c0c8a37e531c35208b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14d8046ae2b46868c687d6743839aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "047ec5980d7542f0a0a6c47c6672859a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b75b74a29ba4235a7959080b8908558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93093a306b534e5da605cdbe70c3116a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install fsspec==2023.9.2\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GAkpUr1n9JkC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpVsQ_DwDQ7Q",
        "outputId": "eb11aea3-3345-4ac1-8182-5f3eea0649fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `1`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForImageTextToText,AutoModelForVision2Seq\n",
        "from peft import LoraConfig\n",
        "base_model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "adapters_name = \"Dtarget/R4llava9JMedF\"\n",
        "\n",
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"bnb_4bit_compute_dtype\": torch.float16\n",
        "}\n",
        "\n",
        "# Load the base model with QLoRA for memory efficiency\n",
        "modelF = AutoModelForVision2Seq.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config  # QLoRA for memory efficiency\n",
        ")\n",
        "\n",
        "\n",
        "modelF = PeftModel.from_pretrained(modelF, adapters_name)\n",
        "modelF = modelF.merge_and_unload()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "98aa254ac2f4444486b76c305d430bc4",
            "5cae6f95410244a286c51090c9203f7f",
            "c798fd277e7a428ba39332a5e38a1371",
            "a41ad3888d7f4569aca9218c1885a791",
            "df79a5d0669d4d7a86bd4ef1b1b30cc3",
            "3e797ec7ef4041eabf20359bb85efb17",
            "dd9acf6193474c0c8a37e531c35208b9",
            "a14d8046ae2b46868c687d6743839aa1",
            "047ec5980d7542f0a0a6c47c6672859a",
            "1b75b74a29ba4235a7959080b8908558",
            "93093a306b534e5da605cdbe70c3116a"
          ]
        },
        "id": "EQLmJiE1Q31u",
        "outputId": "b474a044-6e9f-4488-a7da-47fcad119e7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98aa254ac2f4444486b76c305d430bc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    do_image_splitting=False\n",
        ")"
      ],
      "metadata": {
        "id": "NicVg4rfl-jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3edf9a-9c3c-4baa-9f9e-11694a0a2554"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "yDzr9xkMlgmL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n"
      ],
      "metadata": {
        "id": "_yOymNHThb0V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.dataset[idx]['question']\n",
        "        answer = self.dataset[idx]['answer']\n",
        "        image = self.dataset[idx]['image']  # Assuming it's a PIL image\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"query\": {\"en\": question},\n",
        "            \"answers\": [answer]\n",
        "        }\n",
        "\n",
        "\n",
        "# Define the MyDataCollator class\n",
        "class MyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "        self.image_token = '<image>'\n",
        "        self.end_of_utterance_token = '<end_of_utterance>'\n",
        "        self.pad_token_id = getattr(processor, 'pad_token_id', 0)  # Default to 0 if not found\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        for example in examples:\n",
        "            question = example[\"query\"]['en']\n",
        "            answer = random.choice(example[\"answers\"])\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "                        {\"type\": \"text\", \"text\": self.image_token},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": answer},\n",
        "                        {\"type\": \"text\", \"text\": self.end_of_utterance_token}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "\n",
        "            if isinstance(text, list):\n",
        "                text = \" \".join(str(item) for item in text)\n",
        "            else:\n",
        "                text = str(text)\n",
        "\n",
        "            texts.append(text.strip())\n",
        "\n",
        "        text_batch = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Prepare the batch dictionary\n",
        "        batch = {\n",
        "            \"input_ids\": text_batch[\"input_ids\"],\n",
        "            \"attention_mask\": text_batch[\"attention_mask\"]\n",
        "        }\n",
        "\n",
        "        # Create labels\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == self.pad_token_id] = -100  # Ignore padding in loss calculation\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = VQADataset(dataset=dataset['test'], processor=processor)\n",
        "\n",
        "data_collator = MyDataCollator(processor=processor)"
      ],
      "metadata": {
        "id": "56l6H0_z6q_x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "release_memory(modelF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GRVFZjlRPoK",
        "outputId": "702e0336-5d35-4739-f434-5261e4d6a1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from accelerate.utils import release_memory\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelF.to(device)\n",
        "\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "queries = []\n",
        "sampled_dataset = random.sample(list(test_dataset), 100)\n",
        "# Generate text for each example\n",
        "for example in tqdm(sampled_dataset, desc=\"Generating answers\"):\n",
        "    image = example[\"image\"]\n",
        "    query = example[\"query\"]\n",
        "    ground_truth = example[\"answers\"]\n",
        "\n",
        "    messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "            {\"type\": \"text\", \"text\": processor.image_token},\n",
        "            {\"type\": \"text\", \"text\": query.get('en', '')}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text.strip()],\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        generated_ids = modelF.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=32,  # Very short response\n",
        "                do_sample=False,   # Deterministic outputs\n",
        "                temperature=0.1,   # No randomness\n",
        "                top_p=0.0,         # Focus on top token\n",
        "                repetition_penalty=1.2  # Avoid repetitive outputs\n",
        "            )\n",
        "\n",
        "        release_memory(modelF)\n",
        "        generated_text = processor.batch_decode(\n",
        "            generated_ids[:, inputs[\"input_ids\"].size(1):], skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "    predictions.append(generated_text)\n",
        "    ground_truths.append(ground_truth)\n",
        "    queries.append(query)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Query\": queries,\n",
        "    \"Ground Truth\": ground_truths,\n",
        "    \"Prediction\": predictions\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ldLtjHL08_D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d15153b-0c54-49ab-a24b-998745f89e4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating answers:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   1%|          | 1/100 [00:03<05:34,  3.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   2%|▏         | 2/100 [00:06<04:48,  2.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   3%|▎         | 3/100 [00:07<03:29,  2.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   4%|▍         | 4/100 [00:10<04:00,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   5%|▌         | 5/100 [00:12<04:05,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   6%|▌         | 6/100 [00:15<04:00,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   7%|▋         | 7/100 [00:18<03:56,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   8%|▊         | 8/100 [00:20<03:49,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:   9%|▉         | 9/100 [00:23<03:52,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  10%|█         | 10/100 [00:25<03:39,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  11%|█         | 11/100 [00:27<03:43,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  12%|█▏        | 12/100 [00:30<03:38,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  13%|█▎        | 13/100 [00:33<03:41,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  14%|█▍        | 14/100 [00:35<03:49,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  15%|█▌        | 15/100 [00:38<03:40,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  16%|█▌        | 16/100 [00:40<03:30,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  17%|█▋        | 17/100 [00:43<03:33,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  18%|█▊        | 18/100 [00:46<03:34,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  19%|█▉        | 19/100 [00:49<03:38,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  20%|██        | 20/100 [00:51<03:22,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  21%|██        | 21/100 [00:53<03:09,  2.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  22%|██▏       | 22/100 [00:56<03:15,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  23%|██▎       | 23/100 [00:59<03:32,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  24%|██▍       | 24/100 [01:01<03:04,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  25%|██▌       | 25/100 [01:03<03:09,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  26%|██▌       | 26/100 [01:06<03:12,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  27%|██▋       | 27/100 [01:11<03:57,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  28%|██▊       | 28/100 [01:16<04:29,  3.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  29%|██▉       | 29/100 [01:18<04:02,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  30%|███       | 30/100 [01:21<03:36,  3.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  31%|███       | 31/100 [01:23<03:22,  2.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  32%|███▏      | 32/100 [01:25<02:59,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  33%|███▎      | 33/100 [01:28<02:58,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  34%|███▍      | 34/100 [01:31<02:53,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  35%|███▌      | 35/100 [01:32<02:35,  2.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  36%|███▌      | 36/100 [01:36<02:49,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  37%|███▋      | 37/100 [01:38<02:44,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  38%|███▊      | 38/100 [01:41<02:43,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  39%|███▉      | 39/100 [01:43<02:32,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  40%|████      | 40/100 [01:45<02:24,  2.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  41%|████      | 41/100 [01:51<03:13,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  42%|████▏     | 42/100 [01:53<02:51,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  43%|████▎     | 43/100 [01:55<02:44,  2.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  44%|████▍     | 44/100 [01:58<02:30,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  45%|████▌     | 45/100 [02:03<03:10,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  46%|████▌     | 46/100 [02:06<02:54,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  47%|████▋     | 47/100 [02:08<02:42,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  48%|████▊     | 48/100 [02:11<02:34,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  49%|████▉     | 49/100 [02:14<02:33,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  50%|█████     | 50/100 [02:17<02:25,  2.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  51%|█████     | 51/100 [02:20<02:20,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  52%|█████▏    | 52/100 [02:22<02:15,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  53%|█████▎    | 53/100 [02:25<02:17,  2.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  54%|█████▍    | 54/100 [02:28<02:04,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  55%|█████▌    | 55/100 [02:30<02:00,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  56%|█████▌    | 56/100 [02:32<01:48,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  57%|█████▋    | 57/100 [02:35<01:49,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  58%|█████▊    | 58/100 [02:38<01:55,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  59%|█████▉    | 59/100 [02:40<01:45,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  60%|██████    | 60/100 [02:43<01:42,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  61%|██████    | 61/100 [02:46<01:41,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  62%|██████▏   | 62/100 [02:48<01:38,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  63%|██████▎   | 63/100 [02:51<01:40,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  64%|██████▍   | 64/100 [02:54<01:37,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  65%|██████▌   | 65/100 [02:56<01:30,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  66%|██████▌   | 66/100 [02:58<01:22,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  67%|██████▋   | 67/100 [03:01<01:24,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  68%|██████▊   | 68/100 [03:04<01:22,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  69%|██████▉   | 69/100 [03:06<01:15,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  70%|███████   | 70/100 [03:08<01:13,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  71%|███████   | 71/100 [03:11<01:12,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  72%|███████▏  | 72/100 [03:14<01:11,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  73%|███████▎  | 73/100 [03:16<01:03,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  74%|███████▍  | 74/100 [03:18<01:04,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  75%|███████▌  | 75/100 [03:21<01:00,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  76%|███████▌  | 76/100 [03:23<00:56,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  77%|███████▋  | 77/100 [03:25<00:56,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  78%|███████▊  | 78/100 [03:28<00:53,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  79%|███████▉  | 79/100 [03:31<00:52,  2.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  80%|████████  | 80/100 [03:33<00:51,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  81%|████████  | 81/100 [03:35<00:46,  2.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  82%|████████▏ | 82/100 [03:38<00:45,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  83%|████████▎ | 83/100 [03:41<00:43,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  84%|████████▍ | 84/100 [03:43<00:39,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  85%|████████▌ | 85/100 [03:46<00:37,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  86%|████████▌ | 86/100 [03:48<00:33,  2.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  87%|████████▋ | 87/100 [03:51<00:34,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  88%|████████▊ | 88/100 [03:54<00:32,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  89%|████████▉ | 89/100 [03:56<00:29,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  90%|█████████ | 90/100 [03:59<00:25,  2.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  91%|█████████ | 91/100 [04:02<00:24,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  92%|█████████▏| 92/100 [04:04<00:21,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  93%|█████████▎| 93/100 [04:07<00:17,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  94%|█████████▍| 94/100 [04:09<00:15,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  95%|█████████▌| 95/100 [04:12<00:13,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  96%|█████████▌| 96/100 [04:14<00:09,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  97%|█████████▋| 97/100 [04:17<00:07,  2.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  98%|█████████▊| 98/100 [04:19<00:04,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers:  99%|█████████▉| 99/100 [04:21<00:02,  2.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating answers: 100%|██████████| 100/100 [04:24<00:00,  2.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Jkp4Qf8KhO",
        "outputId": "ac1f8067-f02a-451d-d57e-d0472effa80f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from fuzzywuzzy import fuzz\n",
        "import nltk\n",
        "\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "binary_labels = [1 if \"yes\" in gt.lower() else 0 for gts in ground_truths for gt in gts]  # Convert to binary labels\n",
        "binary_preds = [1 if \"yes\" in pred.lower() else 0 for pred in predictions]  # Convert predictions to binary\n",
        "\n",
        "accuracy_binary = accuracy_score(binary_labels[:len(binary_preds)], binary_preds)\n",
        "\n",
        "print(f\"🌟 Binary Accuracy: {accuracy_binary:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AVxQO6c16wx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59c3908-513d-42b8-dabe-f83cadc9fae5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 Binary Accuracy: 0.6720\n"
          ]
        }
      ]
    }
  ]
}